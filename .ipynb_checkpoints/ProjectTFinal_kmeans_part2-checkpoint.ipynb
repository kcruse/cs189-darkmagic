{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.utils import check_random_state\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_sample(mean, sigma, size):\n",
    "    return np.random.normal(mean, sigma, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: K-means optimizes for intra-cluster distance\n",
    "Running the code block below, you will see that in spite of choosing k=3, the algorithm will often assign 2 centroids to what is actually a single cluster. **Consider the differences in the distributions of data below and explain why this phenomenon might be occurring.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = gaussian_sample([8, 10], [1, 1], [200, 2])\n",
    "class_2 = gaussian_sample([3, 10], [1, 1], [10, 2])\n",
    "class_3 = gaussian_sample([5, 3], [1, 1], [200, 2])\n",
    "X = np.vstack([class_1, class_2, class_3])\n",
    "kmeans = KMeans(n_clusters = 3, init = 'random', n_init = 1).fit(X)\n",
    "\n",
    "plt.scatter(*class_1.T)\n",
    "plt.scatter(*class_2.T)\n",
    "plt.scatter(*class_3.T)\n",
    "plt.scatter(*kmeans.cluster_centers_.T, s = 200, marker = '*', c = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start example-1 answer ####\n",
    "\n",
    "#### End example-1 answer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Random Restarts with K-Means\n",
    "To address the issue in Example 1, we can improve the algorithm's performance by running K-means multiple times with different random initializations. The final chosen clusters will be the one that achieves the smallest cost for the objective function - minimizing the squared sum distances of the data points to the cluster centroids. \n",
    "\n",
    "This is easily implemented using the the sklearn KMeans function with the n_init parameter, which defines the number of random restarts that the algorithm will perform. **Set the value for the n_init parameter in the KMeans function below.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*class_1.T)\n",
    "plt.scatter(*class_2.T)\n",
    "plt.scatter(*class_3.T)\n",
    "## TODO: SET THE N_INIT PARAMETER BELOW ##\n",
    "kmeans = KMeans(n_clusters = 3, init = 'random', n_init = set_value_here).fit(X)\n",
    "## END TODO ##\n",
    "plt.scatter(*kmeans.cluster_centers_.T, s = 200, marker = '*', c = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Feature lifting with K-means\n",
    "Below, we will see how K-means performs on circularly separable data. Running K-means on the original data of 2 concentric circles results in the clustering below. In these demos, each of the colors represent how the algorithm separated the data, where the stars denote the centroids of the clusters. \n",
    "\n",
    "Running ordinary K-means on the original dataset of (x,y) coordinates results in the clustering below. **Comment on the performance below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = datasets.make_circles(noise = 0.05, factor = 0.25)\n",
    "kmeans = KMeans(n_clusters = 2, init = 'random').fit(X)\n",
    "predictions = kmeans.predict(X)\n",
    "colors = ['red', 'blue']\n",
    "plt.scatter(*X.T, c = predictions, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.scatter(*kmeans.cluster_centers_.T, s = 200, marker = '*', c = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Example-3,1 Answer ####\n",
    "\n",
    "#### End Example-3,1 Answer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Polynomial Feature Augmentation \n",
    "As we've seen with regression techniques, we can try lifting the feature space. In this case, a natural thought might be to lift the feature space using the radius-squared feature (summing the squares of the coordinates). **Comment on the performance of this strategy below and why this might result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = np.hstack([X, np.sum(X ** 2, axis = 1).reshape(100,1)])\n",
    "kmeans_lifted = KMeans(n_clusters = 2).fit(X_aug)\n",
    "predictions = kmeans_lifted.predict(X_aug)\n",
    "colors = ['red', 'blue']\n",
    "plt.scatter(*X.T, c = predictions, cmap=matplotlib.colors.ListedColormap(colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Example-3,2 Answer ####\n",
    "\n",
    "#### End Example-3,2 Answer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually selecting an appropriate feature\n",
    "Looking at the feature set above, the radius-squared feature is what would actually help us differentiate between the clusters. Let us try running K-means again using only the radius-squared feature. **Comment on the performance of this strategy below and why this might result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = np.sum(X ** 2, axis = 1)\n",
    "kmeans_lifted = KMeans(n_clusters = 2).fit(X_aug.reshape(-1,1))\n",
    "predictions = kmeans_lifted.predict(X_aug.reshape(-1,1))\n",
    "colors = ['red', 'blue']\n",
    "plt.scatter(*X.T, c = predictions, cmap=matplotlib.colors.ListedColormap(colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Example-3,3 Answer ####\n",
    "\n",
    "#### End Example-3,3 Answer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Kernel PCA to lift & reduce the dimensionality\n",
    "We cannot always manually construct and select the feature that will help separate the data. In this case, we will use KernelPCA with the RBF Kernel. This allows us to perform non-linear dimensionality reduction. We can then run KMeans in this reduced space to cluster the data.\n",
    "\n",
    "The first graph shows the projection of data points into 2-D PCA space. The second graph visualizes how the original data points were clustered using the PCA + K-means strategy. **Comment on the results of this strategy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: TUNE A VALUE OF GAMMA THAT CREATES SEPARABLE CLUSTERS FOR K-MEANS ##\n",
    "kpca = KernelPCA(n_components = 2, kernel = 'rbf', gamma = 5)\n",
    "## END TODO ##\n",
    "\n",
    "projected_data = kpca.fit_transform(X)\n",
    "plt.scatter(*projected_data.T, c = Y, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "print(\"Points projected into PCA Space\")\n",
    "kmeans_pca = KMeans(n_clusters = 2).fit(projected_data)\n",
    "predictions = kmeans_pca.predict(projected_data)\n",
    "plt.scatter(*kmeans_pca.cluster_centers_.T, s = 200, marker = '*', c = 'black')\n",
    "plt.show()\n",
    "\n",
    "print(\"Output of Clustering Algorithm in Original Space\")\n",
    "plt.scatter(*X.T, c = predictions, cmap=matplotlib.colors.ListedColormap(colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Example-3,3 Answer ####\n",
    "\n",
    "#### End Example-3,3 Answer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations of k-Means: Kernelized k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how kernels are a more efficient way for model calculations through the computation of inner products. Please read through Section 2.1 in the paper below to understand how kernelized k-Means works, particularly focusing on the update step. Below is a FLAWED implementation of kernelized k-Means. Please locate the bugs and correct them so that the function works as intended.\n",
    "\n",
    "Link to paper: https://www.cs.utexas.edu/users/inderjit/public_papers/kdd_spectral_kernelkmeans.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_kernelized(data, k, max_iter = 1000, weights = None, kernel_type = \"linear\"):\n",
    "    n = len(data)\n",
    "    # Randomize initial clusters\n",
    "    random_state = check_random_state(None)\n",
    "    labels = np.random.randint(k, size = n)\n",
    "\n",
    "    # Initialize kernel\n",
    "    kernel = pairwise_kernels(data, None, kernel_type)\n",
    "\n",
    "    # Set weights if not defined to be all 1s (no preference)\n",
    "    if not weights:\n",
    "        weights = np.ones(n)\n",
    "\n",
    "    # Inter/Intra Cluster distance storage\n",
    "    intra_dist = np.zeros((n, k))\n",
    "    inter_dist = np.zeros(k)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        intra_dist.fill(0)\n",
    "        for cluster in range(k):\n",
    "            # This is our mask to isolate the points currently in cluster j\n",
    "            cluster_points = labels == cluster\n",
    "\n",
    "            # If k is too large, we may have empty clusters!\n",
    "            if np.sum(cluster_points) == 0:\n",
    "                return \"Empty cluster!\"\n",
    "\n",
    "            size = np.count_nonzero(cluster_points)\n",
    "            cluster_kernel = kernel[cluster_points][:, cluster_points]\n",
    "\n",
    "            # This is our distance update step using the kernel trick!\n",
    "            this_cluster_dist = np.sum(np.outer(weights[cluster_points], weights[cluster_points]) * cluster_kernel / size ** 2)\n",
    "            inter_dist[cluster] = this_cluster_dist\n",
    "\n",
    "            # Update step\n",
    "            intra_dist[:, cluster] += this_cluster_dist \n",
    "            intra_dist[:, cluster] -= 2 * np.sum(weights[cluster_points] * kernel[:, cluster_points], axis = 1) / size\n",
    "\n",
    "    # If none of the labels were updated, we are done.  \n",
    "        previous_labels = labels[:]\n",
    "        labels = intra_dist.argmin(axis = 1)\n",
    "        if np.sum((labels - previous_labels) == 0) == 0:\n",
    "            break\n",
    "\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test the function. Random 2D data is generated in this simple case.\n",
    "data = 100 * np.random.rand(100, 2)\n",
    "colors = {0:'red', 1:'green', 2:'blue', 3:'yellow', 4: \"orange\", 5: 'purple', 6: 'black', 7: 'pink'}\n",
    "plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the function is correctly fixed, distinct clusters should be seen.\n",
    "labels = k_means_kernelized(data, 5, 1000)\n",
    "color_labels = np.vectorize(colors.get)(labels)\n",
    "color_labels\n",
    "plt.scatter(data[:, 0], data[:, 1], c = color_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Try the kernelized clustering with different parameters. What happens when:\n",
    "        \n",
    "<ul>    \n",
    "<li>The number of clusters is reduced from 5 to 3?</li>\n",
    "<li>An RBF kernel is used instead of a linear one?</li>\n",
    "<li>max_iter is reduced to 100 or increased to 10000?</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
