{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain-specific task: classifying polymers # \n",
    "\n",
    "In this exercise, you are tasked with performing k-means clustering on dataset of polymer properties. This exercise should give you a practical understanding of how to implement k-means as well as exposure to one flavor of the many kinds of tasks you will be asked to perform in internship settings. It is important to have an appreciation for the \"big picture\" when it come to data science and what you are actually trying to achieve, regardless of domain.\n",
    "\n",
    "<ins>Motivation</ins>: You are working for a materials supplier and your boss needs to know what kind of polymer will work best in a given situation (maybe for a playground slide). It is important that the polymer of choice withstand high temperatures without deforming easily under pressure. A material's ability to resist deformation is measured by its Elastic and Shear Moduli (Elastic being relevant in settings of tension, Shear being relevant in settings of torsion or shearing). Shearing will be important here given the way that children will use the slide. Polymers also contain a specific material property known as the Glass Transition Temperature. This temperature determines the limit at which the polymer's stiffness (or, ability to resist deformation) breaks down. You know that polymers can typically be described by three categories, delineated most clearly by their degree of cross-linking (i.e. the density of chemical bonds between adjacent molecular polymer strands) and their ability to deform without permament damage:\n",
    "\n",
    "<ul>    \n",
    "<li>Thermosets (high cross-linking, high resistance to deformation)</li>\n",
    "<li>Thermoplastics (no cross-linking, moderate resistance to deformation)</li>\n",
    "<li>Elastomers (low to moderate cross-linking, low resistance to deformation, but no permanent deformation)</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "Note that polymers are usually not easily binned into these three categories in the real world since they can often exhibit characteristics of two or all three types.\n",
    "\n",
    "<ins>Task</ins>: Your advisor has a dataset of polymers, but they do not have the names of these polymers or their types (this might be a common case if the materials developed are highly secretive). The dataset includes the values of their shear moduli, glass transition temperature, density, the precision of the density measurements, and 3 categorical variables: (1) whether the polymer melts or burns (a distinguishing factor for thermosets is that they burn rather than melt), (2) the degree of cross-linking (thermoplastics have no cross-linking, elastomers have low to moderate, and thermosets have high cross-linking), and (3) the degree of elongation (i.e. how far the material can be stretched before deforming permanently). With only this dataset, use k-means to systematically separate the data points into some optimal number of distinct clusters.\n",
    "\n",
    "Run each of the cells below, adding in your own code where required. Write down your answers/comments for the following short answer questions, as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data visualization ##\n",
    "\n",
    "Let's take a look at the dataset your boss gave you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polymers_df = pd.read_csv(\"./PolymersData_NoLabels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polymers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a could of helper functions which we will use throughout. Let's plot the shear modulus against the glass transition temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, x, y, hue=None, logscale=False):\n",
    "    sns.set_style('whitegrid')\n",
    "    fig = sns.lmplot(x=x, y=y, \n",
    "               data=df, hue=hue, palette='coolwarm',\n",
    "               height=6, aspect=1, fit_reg=False)\n",
    "\n",
    "    ax = fig.axes[0][0]\n",
    "    if logscale:\n",
    "        ax.set_yscale('log')\n",
    "    return\n",
    "\n",
    "def df2np(df, xs):\n",
    "    X = df[xs].to_numpy()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df=polymers_df, \n",
    "          x='Glass Transition Temperature (K)', \n",
    "          y='Shear Modulus (MPa)',\n",
    "          logscale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not give us much intuition in terms of separating data points. However, it looks like there are a group of points clustered at very low shear moduli. Let's try to plot the shear modulus on a log scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df=polymers_df, \n",
    "          x='Glass Transition Temperature (K)', \n",
    "          y='Shear Modulus (MPa)',\n",
    "          logscale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are starting to see some significance in the separation between the data points. There are still several features which we have not utilized, though. Use the 'plot_data' helper function to produce two other plots and visualize the Glass Transition Temperature as well as the Shear Modulus against the Density. Do you see any significant clusters in the data? (Remember you can plot on a log scale if you think that will help illuminate any trends better).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use plot_data to plot the remaining numerical features\n",
    "# from the dataset \n",
    "#### start_plotting ####\n",
    "\n",
    "#### end_plotting ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comment on the visualizations above. Is it easy to isolate any clusters from one another? What features have we not looked at yet?__\n",
    "\n",
    "#### Start Comments ####\n",
    "\n",
    "#### End Comments ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use PCA to reduce the dimensionality of the dataset and to utilize all of the remaining features. We have mapped the categorical data to numerical values. With this new dataframe, we can use PCA to reduce the dimensionality of the dataset and hopefully isolate all of the datapoints into regions of high similarity. Run the cell below to see this visualized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_pca(x):\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(data = principalComponents,\n",
    "                               columns = ['principal component 1', 'principal component 2'])\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.scatter(principalDf['principal component 1'], principalDf['principal component 2'], s=50)\n",
    "    ax.grid()\n",
    "    return principalDf\n",
    "\n",
    "cat2num = {'Melt or Burn?' : {'melt' : 0, 'burn' : 1},\n",
    "           'Level of Cross-Linking' : {'none' : 0, 'low' : 1, 'moderate' : 2, 'high' : 3},\n",
    "           'Elongation' : {'low' : 0, 'moderate' : 1, 'high' : 2}}\n",
    "\n",
    "\n",
    "polymers_df_norm = polymers_df.replace(cat2num)\n",
    "x = polymers_df_norm.loc[:, features].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "features = ['Glass Transition Temperature (K)', 'Shear Modulus (MPa)', 'Density (g/cm^3)', 'Level of Cross-Linking', 'Melt or Burn?', 'Elongation']\n",
    "principalDf = implement_pca(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll map this dataframe to a numpy array for ease of computation for the remainder of the notebook. Run the cell below before moving onto the next part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2np(df=principalDf, xs=['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: k-Means Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ denote the set of $N$ data points $\\vec{x_i} \\in \\mathbb{R}^d$. A cluster assignment is a partition $C_1, ..., C_k \\in X$ such that the sets $C_k$ are disjoint and $X = C_1 \\cup ... \\cup C_k$. A data point $\\vec{x_i} \\in X$ is said to belong to cluster $k$ if it is in $C_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, k-Means solves the following optimization problem: $\\underset{\\{C_k\\}_{k = 1}^{K}, \\{\\vec{c_k}\\}_{k = 1}^{K}: X = C_1 \\cup ... \\cup C_K}{\\arg\\min} \\sum_{k = 1}^{K} \\underset{\\vec{x} \\in C_k}{\\sum} \\|\\vec{x} - \\vec{c_k}\\|^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More informally, k-Means is trying to find the best cluster centers & assignments that maximize the distance between clusters and minimize the size of each cluster. In other words, we want to find clusters that are as far apart (distinct) from each other as possible, while making sure that all the points in a cluster are pretty close to each other. \n",
    "\n",
    "Solving this optimization problem directly is NP-hard (i.e. computationally very difficult). Instead, we use Lloyd's Algorithm: \n",
    "1. Pick some K representing the number of groups you wish to cluster your datset into\n",
    "2. Randomly initialize $\\vec{c_k}, k = 1,...,K$\n",
    "3. While $\\vec{c_k}$ keeps changing (k-Means has not converged): \n",
    "    * Update partition $C_1 \\cup ... \\cup C_k$ given by the $\\vec{c_k}$ by assigning each $\\vec{x_i} \\in X$ to the cluster represented by its nearest centroid: $\\underset{k}{\\arg\\min} \\|\\vec{x} - \\vec{c_k}\\|^2$\n",
    "    * Update centroid $\\vec{c_k}$ given $C_1 \\cup ... \\cup C_k$ as $\\vec{c_k} = \\frac{1}{\\lvert {C_k} \\rvert} \\sum_{\\vec{x} \\in C_k} \\vec{x}$\n",
    "\n",
    "This algorithm will always converge to some value because each step in (3) is guaranteed to not increase the value of the objective function. This two-step algorithm can be implemented using the help of two functions: \n",
    "1. assign_clusters: given a set of clusters, assign each data point to the best cluster\n",
    "2. update_means: given a set of cluster assignments, compute the new best cluster center (a.k.a. centroid)\n",
    "\n",
    "#### TODO: complete the implementations of assign_clusters and update_means below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(X, c): \n",
    "    \"\"\"\n",
    "    Takes in an (n x d) data matrix X, and a (k x d) matrix of the centroids c. \n",
    "    Returns a length-n vector with the index of the closest centroid to each data point.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    k = c.shape[0]\n",
    "    assert d == c.shape[1], \"Centroids are of the wrong shape\"\n",
    "    assignments = np.zeros(n)\n",
    "    for i, point in enumerate(X): \n",
    "        #TODO: Set assignments[i] to be the index of the centroid who is closest to the current point\n",
    "        ### start assign_cluster ###\n",
    "        ############################\n",
    "\n",
    "        ###########################\n",
    "        ### end assign_cluster ###\n",
    "    return assignments\n",
    "\n",
    "def update_means(X, assignments): \n",
    "    \"\"\"\n",
    "    Takes in an (n x d) data matrix X, and a length-n vector of\n",
    "    the cluster indices of each point.\n",
    "    Computes the mean of each cluster and returns a (k x d) matrix of the centroids.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    assert len(assignments) == n\n",
    "    k = len(set(assignments))\n",
    "    centroids = []\n",
    "    for i in range(k): \n",
    "        #TODO: set centroid_i to be the mean of all points in cluster 'i'\n",
    "        ### start update_means ###\n",
    "\n",
    "        ### end update_means ###\n",
    "        centroids.append(centroid_i)\n",
    "    return np.array(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with assign_clusters and update_means, we need a few other helper functions to have a working k-Means implementation: \n",
    "1. initialize_centroids: this will randomly pick k number of data points to serve as the initial centroids to pass into assign_clusters to kickstart the algorithm. \n",
    "2. cost: unlike supervised learning problems where we can measure training error by calculating the distance between our model output and the true output, unsupervised learning does not have the same luxury. We actually don't have access to the true clusters that the data points belong to, so we have to come up with a different measure of how well or poor our clustering is. For this demo, we will use the summ of squared distance of each data point to its centroid as a measure of cost. \n",
    "\n",
    "#### TODO: Complete the implementations of initialize_centroids and cost below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centroids(X, k): \n",
    "    \"\"\"\n",
    "    Takes in an (n x d) data matrix X, and employs forgy initialization, where the initialized centroids are\n",
    "    randomly chosen from the input data set. \n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    #TODO: Set centroids to be k randomly picked data points from X\n",
    "    ### start initialize_centroids ###\n",
    "    ##################################\n",
    "\n",
    "    ################################\n",
    "    ### end initialize_centroids ###\n",
    "    return centroids\n",
    "\n",
    "def cost(X, assignments, centroids): \n",
    "    \"\"\"\n",
    "    Computes the sum of the squared distance between each point\n",
    "    and the mean of its associated cluster. \n",
    "    Remember: k-Means seeks to minimize intra-cluster distance \n",
    "    and maximize inter-cluster distance.\n",
    "    \"\"\"\n",
    "    total_cost = 0\n",
    "    n, d = X.shape\n",
    "    k = centroids.shape[0]\n",
    "    assert centroids.shape[1] == d\n",
    "    assert len(assignments) == n\n",
    "    for i in range(k): \n",
    "        #TODO: set cost_i to be the sum of squared distance from all points in cluster 'i' to centroid 'i'\n",
    "        ### start cost ###\n",
    "        ##################\n",
    "\n",
    "        ################\n",
    "        ### end cost ###\n",
    "        total_cost += cost_i\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these functions together will give us a working k-Means implementation. We've provided a function that will run through Lloyd's algorithm and call each of these functions individualy. We've also included a few more functions that will help us visualize our results. No need to modify any of the functions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(X, k): \n",
    "    \"\"\"\n",
    "    Take in an (n x d) data matrix X and a parameter 'k' representing the number of clusters. \n",
    "    Yields the centroids and cluster assignments after each step \n",
    "    of running k-means in a 2-tuple.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    assignments = assign_clusters(X, centroids)\n",
    "    \n",
    "    steps = []\n",
    "    \n",
    "    while True: \n",
    "        steps.append((centroids, assignments))\n",
    "        centroids = update_means(X, assignments)\n",
    "        new_assignments = assign_clusters(X, centroids)\n",
    "        if np.all(assignments == new_assignments): \n",
    "            steps.append((centroids, assignments))\n",
    "            error = cost(X, assignments, centroids)\n",
    "#             print(f'Final cost = {cost(X, assignments, centroids)}')\n",
    "            break\n",
    "        assignments = new_assignments\n",
    "    \n",
    "    return steps, error\n",
    "\n",
    "def final_k_means_cluster(X, k): \n",
    "    steps, cost = k_means(X, k)\n",
    "    return steps[-1][0], steps[-1][1], cost\n",
    "\n",
    "def plot_clustering(X, centroids, assignments): \n",
    "    k = len(centroids)\n",
    "    for j in range(k): \n",
    "        plt.scatter(*X[assignments == j].T)\n",
    "    plt.scatter(*centroids.T, marker='x', s = 240, c = 'black')\n",
    "    plt.show()\n",
    "\n",
    "def interact_clustering(X, logger): \n",
    "    history = list(logger)\n",
    "    k = history[0][0].shape[0]\n",
    "    \n",
    "    def plotter(i): \n",
    "        plot_clustering(X, *history[i])\n",
    "    \n",
    "    interact(plotter, i = IntSlider(min=0, max=len(history) - 1, continuous_update = False))\n",
    "\n",
    "def demo(classes, history= False): \n",
    "    for c in classes: \n",
    "        plt.scatter(*c.T)\n",
    "    plt.show()\n",
    "    \n",
    "    points = np.vstack(classes)\n",
    "    \n",
    "    if history: \n",
    "        interact_clustering(points, k_means_cluster(points, len(classes)))\n",
    "    else: \n",
    "        centroids, assignments = final_k_means_cluster(points, len(classes))\n",
    "        plot_clustering(points, centroids, assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can continue, we should make sure that our implementation is working properly. Let's compare the results of our k-Means implementation with Sklearn's implementation on our dataset X projected onto its first 2 principal components. This way, we can actually visualize the clusters in 2D space. If we were to work with the raw dataset in dimensions greater than 3, we couldn't visualize the clusters & would have to rely on our measure of cost to determine how effective the clusters were. \n",
    "\n",
    "#### TODO: Referencing the documentation for the sklearn implementation of K-Means, fill in the necessary code into the following helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_implementation(X, k):\n",
    "    \"\"\"\n",
    "    Takes in an (n x d) matrix X and a parameter 'k' representing the number of clusters. \n",
    "    Returns the final (centroid, assignments) tuple.\n",
    "    \"\"\"\n",
    "    #TODO: set the centroids, assignments and cost variables of the function\n",
    "    kmeans = KMeans(n_clusters=k, n_init=1, init=initialize_centroids(X, k)).fit(X)\n",
    "    ### start package_implementation ###\n",
    "    ####################################\n",
    "\n",
    "    ##################################\n",
    "    ### end package_implementation ###\n",
    "    \n",
    "    return (centroids, assignments, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, assignments, total_distance_cost = final_k_means_cluster(X, k=5)\n",
    "print(total_distance_cost)\n",
    "plot_clustering(X, centroids, assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, assignments, total_distance_cost = package_implementation(X, k=5)\n",
    "print(total_distance_cost)\n",
    "plot_clustering(X, centroids, assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your prediction match the results of the demo? Why or why not? \n",
    "#### Start Comments ####\n",
    "\n",
    "#### End Comments ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tuning our k-Means Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things we could do to adjust our k-Means implementation to get different clustering results. We'll focus on two different tactics for finding the best parameters to feed into our k-Means implementation: \n",
    "1. The number of clusters, K: this will determine how many classes we're left with after our data is clustered. Traditionally, this is either picked using domain knowledge, cross-validation, or a little bit of both. \n",
    "2. The dimensionality of data: we can choose to either pass in the raw data, or we can reduce the dimensions of the data. Sometimes, reducing the dimensionality can yield much better clusters because we're only concerning ourselves with the most important components of the data. We saw earlier that projecting the data onto its first two principal components made the data visualization look really different. We can see how clustering might change if we change the number of principal components we project onto. \n",
    "\n",
    "Let's explore a how changing the target number of clusters would change the outcome of our implementation. \n",
    "\n",
    "What range for K would be appropriate for this hyperparameter tuning task? \n",
    "\n",
    "#### Start Answer ####\n",
    "\n",
    "#### End Answer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good sanity check would be to look at the clusters created for different values of k.\n",
    "\n",
    "#### TODO: change num_clusters to visualize the output of k-Means for different values of k when projecting onto d=2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "centroids, assignments, total_distance_cost = final_k_means_cluster(X, k=num_clusters)\n",
    "plot_clustering(X, centroids, assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, the number of dimensions, d, would be another tuneable hyperparameter. However, we can't perform the same sanity check to look at which projections give us the best clusters because we can't visualize dimensions beyond 3D. Typically, in practice, it's difficult to determine which dimension to project the data into and many data scientists often do this by trial and error. For the purposes of our project, your boss tells you to work in 3 dimensions so that you can easily visualize the different clusters that form. \n",
    "\n",
    "#### TODO: pick values for max_k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 10\n",
    "prospective_k = range(1, max_k + 1)\n",
    "\n",
    "d = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_k_means(k, d, X): \n",
    "    pca = PCA(n_components=d)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    return final_k_means_cluster(principalComponents, k=k)\n",
    "\n",
    "def plot_clustering_d(X, d, centroids, assignments): \n",
    "    pca = PCA(n_components=d)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    k = len(centroids)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for j in range(k): \n",
    "        ax.scatter(*principalComponents[assignments == j].T)\n",
    "    ax.scatter(*centroids.T, marker='x', s = 240, c = 'black')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def validation(arr_k, d, X): \n",
    "    all_centroids = []\n",
    "    all_assignments = []\n",
    "    all_costs = np.zeros((len(arr_k)))\n",
    "    for i in range(0, len(arr_k)): \n",
    "        centroids, assignments, total_distance_cost = perform_k_means(arr_k[i], d, X)\n",
    "        all_centroids.append(centroids)\n",
    "        all_assignments.append(assignments)\n",
    "        all_costs[i] = total_distance_cost\n",
    "    \n",
    "    return all_costs, all_centroids, all_assignments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "costs, centroids, assignments = validation(prospective_k, d, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, when we pick hyperparameters in supervised learning problems, we would separate our dataset into training & validation sets, then test the model created from the training set with different hyperparameters on the validation set. We expect to see a parabola-shaped validation error graph and we pick the hyperparameters that minimize this graph. However, this is not possible with an unsupervised learning problem like K-Means because we do not have any ground truth labels to compare against. Instead, our cost function will always decline as the number of clusters increases because more clusters will always decrease the distance from a data point to its nearest cluster. Therefore, to prevent overfitting to the dataset, data scientists will typically choose the cluster number at the \"elbow\" of the cost graph. The elbow method is a heuristic used to pick the best cluster size to choose a point where the diminishing returns of more clusters are no longer worth the additional cost. \n",
    "\n",
    "#### TODO: Plot the costs against all the prospective k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prospective_k, costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, what is the optimal value of k? Also, report the min and max costs of clustering. \n",
    "\n",
    "#### Start Answer ####\n",
    "\n",
    "#### End Answer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You take your findings to your boss. They tell you they just received more information about the polymers contained in the dataset you've been working with. The polymers are guaranteed to fall into 3 different categories: thermosets, thermoplastics, and elastomers. Based on this information, does your choice of the number of clusters change? \n",
    "\n",
    "#### Start Answer ####\n",
    "\n",
    "#### End Answer ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the reduced data with the confirmed number of cluster assignments below in 3 dimensions. If you would like to interact with the 3D plot, feel free to uncomment the relevant line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill in the value for best_k to visualize the clustering.\n",
    "# %matplotlib qt UNCOMMENT THIS LINE TO INTERACT WITH THE 3-D GRAPH (zoom in/out, rotate, etc)\n",
    "best_k = 3\n",
    "plot_clustering_d(x, 3, centroids[best_k-1], assignments[best_k-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've made it to the end of Part 1 of the project. This notebook was intended to help you understand how K-Means would be implemented in an industry setting from end-to-end: \n",
    "1. Loading & cleaning the dataset\n",
    "2. Visualizing the data\n",
    "3. Implementing K-Means\n",
    "4. Tuning the hyperparameters associated with the clustering model. \n",
    "\n",
    "Now, please move on to Part 2 of the project where you will get to explore the ins and outs of K-Means more in-depth with some toy examples. \n",
    "\n",
    "(Credit for some of the code above is extended to the contributors to the CS189 discussion notes for clustering: https://www.eecs189.org/static/discussions/dis9.pdf?fbclid=IwAR2oE4djIUQW3sYfkDTw3RaMqtZOy-pno4O_9_wQOiEgim1LLBpxAGfxcfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
